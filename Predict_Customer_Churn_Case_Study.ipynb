{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Predict Customer Churn Case Study.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ANILESLAVATH12/customer-churn/blob/main/Predict_Customer_Churn_Case_Study.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "jrPuC4gOy0o-"
      },
      "cell_type": "markdown",
      "source": [
        "# Case Study: Predict Customer Churn Using Machine Learning\n",
        "Prepared by Michelle Bonat"
      ]
    },
    {
      "metadata": {
        "id": "fi-v3NSWy0o_"
      },
      "cell_type": "markdown",
      "source": [
        "## 1: Frame the Problem"
      ]
    },
    {
      "metadata": {
        "id": "QRTQfLFty0pB"
      },
      "cell_type": "markdown",
      "source": [
        "## **Case Study Overview**\n",
        "\n",
        "Many companies would benefit immensely by investigating their customer churn and taking steps to improve it. Why? Simply stated, it usually takes much less resources to keep a customer than to get a new one. So if you have the information you need on why customers are leaving (churning) you can use this proactively to reduce your churn. Let's look at how we can develop this intelligence using machine learning.\n",
        "\n",
        "----\n",
        "##About The Data and The Approach\n",
        "\n",
        "We're told by our colleagues at the hypothetical company that customer churn is at 50% within 3 months. That means that within 3 months of a set of customers that sign up for the paid product, by the end of 3 months half of them will have cancelled. This is an urgent problem we need to help fix with machine learning!\n",
        "\n",
        "## - Dataset:\n",
        "For this dataset I used an IBM Sample Dataset for customer churn from the TelCo industry which can be found [here](https://community.watsonanalytics.com/wp-content/uploads/2015/03/WA_Fn-UseC_-Telco-Customer-Churn.csv)\n"
      ]
    },
    {
      "metadata": {
        "id": "lJ2xdJLKltO1"
      },
      "cell_type": "markdown",
      "source": [
        "## - Project Steps:\n",
        "\n",
        "1. **Talk with stakeholders (S/H) to: **\n",
        "  * **Discuss/determine goals and metrics for the project.**  Is reducing churn the right problem to solve? How specifically shall we measure performance improvement? What is our success goal? (Assume here: it is the right problem, we measure performance overall by reducing customer churn, success is reducing customer churn by 10% in next 6 months).\n",
        "  * **Understand what deliverables are useful for internal stakeholders** (Assume it is churn prediction factors, later a spreadsheet of customer churn predictions, production pipeline and perhaps an internal dashboard).\n",
        "  * **Discuss an iterative approach with timing and deliverables**, at a high level. First see if the data is predictive. Set a goal for what predictive means (Assume here: 80% +). Come back with findings in 2 weeks.\n",
        "2. **Work with SME's (Subject Matter Experts) to come up with a hypothesis.** Here we'll assume that tenure, addt'l products/features, tech support interactions are key; also possibly how they pay.\n",
        "3. **Get the data.** Work with data engineering to get the data, if needed.\n",
        "3. **Talk to my team and resource the project.** Include team personal skills development where possible. In this case for example purposes, assume all the team is booked and I'll do the initial analysis which appears here.\n",
        "3. **Prototype some approaches.** Is the data usable? Can churn be reliably predicted? Do we need other data? Can it eventually be done at scale?\n",
        "4. **When a valid prototype is achieved, begin A/B testing.** If the prototype indicates a workable approach is viable, coordinate with S/H and team on next steps. Put A/B testing in place.\n",
        "5. **Iterate data and models learned from A/B testing and internal model testing.** Get to target success metrics on prediction rate and any other target metrics in order to move forward.\n",
        "6. **Create the production pipeline. **Once an initial model prototype has been established that surpasses the desired performance thresholds, it is ready for the production line.\n",
        "  * Start simple with incremental wins. Expland as it makes sense with priorities and resources.\n",
        "    * Load the flagged users (potential churners) back to the database as a first step\n",
        "    * Eventually, move to automated data injection from a data warehouse\n",
        "    * Refine the model and the data\n",
        "    * Report performance to stakeholders and team on an ongiong basis. Get feedback and discuss next steps. Implement, learn, improve, repeat.\n",
        "7. **Suggest some ways to improve churn in products and processes based on learnings from the activities listed above.**\n",
        "    * These notes are listed at the bottom of this notebook.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "olnOnYIcOnWr"
      },
      "cell_type": "markdown",
      "source": [
        "## 2: Data we would we like to have:\n",
        "\n",
        "Generally speaking we would like to have as much data as possible with a long history over as many categories as possible.\n",
        "\n",
        "Here are some types of data that are useful in customer churn analysis:\n",
        "* Customer ID or other identification information\n",
        "* Date the customer was acquired\n",
        "* How the customer was acquired (source of sale i.e. referral, web signup, etc.)\n",
        "* Plan type (what subscription they are on)\n",
        "* Cohort analysis by user type (seasonal onboards by marketing campaign or time of year, etc.)\n",
        "* If they use add ons (sush as Online Security or Device Protection)?\n",
        "* Have they set up to pay for their subscription online?\n",
        "* Customer size\n",
        "* Customer segment type (i.e. company user, accountant)\n",
        "* Customer country of residence\n",
        "* Customer state of residence\n",
        "* NPS score (satisfaction level 0-10 from questionnaire)\n",
        "* Time to first success moment (days)\n",
        "* Total number of times logged in\n",
        "* Time since last login\n",
        "* Days since key inflection points (work with SME in marketing/product) this could be days since logging in, since getting their first result from the technology, etc.\n",
        "* Time spent logged in past month\n",
        "* Time spent logged in average/mo for length of subscription\n",
        "* Number of times they have contacted customer service over life of subscription\n",
        "* Number of customer service calls in the prior month that they cancelled\n",
        "* Number of times they opened and clicked on the Help text in the app or online\n",
        "* What they typed into the search box in the help text in the app\n",
        "* Tenure (Lifetime account duration in days)\n",
        "* Total subscription amount paid\n",
        "* Date unsubscribed (timestamp)\n",
        "* How many other products they have from our company\n",
        "* What other products they have from our company, as a separate column for each (yes / no)\n",
        "* Anything else specific to the business of note\n",
        "* Cancelled yes or no (Churn)"
      ]
    },
    {
      "metadata": {
        "id": "2lYY3102y0pC"
      },
      "cell_type": "markdown",
      "source": [
        "## 3: SetUp: Importing the Libraries"
      ]
    },
    {
      "metadata": {
        "id": "H61EBZgvy0pD"
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import missingno as ms\n",
        "from sklearn import model_selection, metrics  #to include metrics for evaluation # this used to be cross_validation\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pylab as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2_wZKe4DMfod"
      },
      "cell_type": "code",
      "source": [
        "# Quiet warnings since this is a demo (it quiets future and deprecation warnings).\n",
        "def warn(*args, **kwargs):\n",
        "    pass\n",
        "import warnings\n",
        "warnings.warn = warn"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sknHgWZQy0pI"
      },
      "cell_type": "markdown",
      "source": [
        "## 4: Access Data and Clean It Up"
      ]
    },
    {
      "metadata": {
        "id": "OwHp561YCO-2"
      },
      "cell_type": "markdown",
      "source": [
        "We're using the IBM Telco sample dataset for this case study. It provides some of the datapoints from our wish list but not all.\n",
        "\n",
        "I staged the IBM sample dataset on my own S3 bucket on AWS and I'm accessing it from there:"
      ]
    },
    {
      "metadata": {
        "id": "fzVASp4_jCrd",
        "outputId": "453ab9e5-aec1-4b35-dc0b-bb90ebb82695",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "!wget https://s3.amazonaws.com/ibm-sample-cust-churn-data/WA_Fn-UseC_-Telco-Customer-Churn.csv"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-03-05 09:58:48--  https://s3.amazonaws.com/ibm-sample-cust-churn-data/WA_Fn-UseC_-Telco-Customer-Churn.csv\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 3.5.17.29, 16.182.67.104, 52.217.74.86, ...\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|3.5.17.29|:443... connected.\n",
            "HTTP request sent, awaiting response... 404 Not Found\n",
            "2025-03-05 09:58:48 ERROR 404: Not Found.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "dmOFyHcF00bR",
        "outputId": "82ae1a2a-3bcb-411e-ddd4-53482164fd80",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "ls -l"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 4\n",
            "drwxr-xr-x 1 root root 4096 Mar  3 14:19 \u001b[0m\u001b[01;34msample_data\u001b[0m/\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "gfmhfLvmy0pJ",
        "outputId": "b7cd1217-5210-49f5-f53c-01feb7f9e27b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        }
      },
      "cell_type": "code",
      "source": [
        "# Read the data and view the top portion to see what we are dealing with.\n",
        "data=pd.read_csv('WA_Fn-UseC_-Telco-Customer-Churn.csv')\n",
        "data.head()"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'WA_Fn-UseC_-Telco-Customer-Churn.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-65-d2037b11bc06>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Read the data and view the top portion to see what we are dealing with.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'WA_Fn-UseC_-Telco-Customer-Churn.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'WA_Fn-UseC_-Telco-Customer-Churn.csv'"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "yQJYcrjpy0pN"
      },
      "cell_type": "code",
      "source": [
        "# See if the data is usable.\n",
        "data.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bvtnGr4PlRew"
      },
      "cell_type": "code",
      "source": [
        "# Analyze if there is non-numeric data in the 'TotalCharges' column since it's showing as an object instead of float64.\n",
        "data['TotalCharges'] = pd.to_numeric(data['TotalCharges'], errors = 'coerce')\n",
        "data.loc[data['TotalCharges'].isna()==True]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dq4060fjmT-C"
      },
      "cell_type": "code",
      "source": [
        "# Above we see that the blank \"TotalCharges\" happen when customers have 0 months tenure so we will change those values to $0.\n",
        "data[data['TotalCharges'].isna()==True] = 0\n",
        "data['OnlineBackup'].unique()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "A2E-8kDo0UxF"
      },
      "cell_type": "code",
      "source": [
        "# See how many rows and columns.\n",
        "data.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CxUAu1yBm02V"
      },
      "cell_type": "markdown",
      "source": [
        "More data cleanup: next we’ll convert the categorical values into numeric values."
      ]
    },
    {
      "metadata": {
        "id": "R6etPuXjm_BL"
      },
      "cell_type": "code",
      "source": [
        "data['gender'].replace(['Male','Female'],[0,1],inplace=True)\n",
        "data['Partner'].replace(['Yes','No'],[1,0],inplace=True)\n",
        "data['Dependents'].replace(['Yes','No'],[1,0],inplace=True)\n",
        "data['PhoneService'].replace(['Yes','No'],[1,0],inplace=True)\n",
        "data['MultipleLines'].replace(['No phone service','No', 'Yes'],[0,0,1],inplace=True)\n",
        "data['InternetService'].replace(['No','DSL','Fiber optic'],[0,1,2],inplace=True)\n",
        "data['OnlineSecurity'].replace(['No','Yes','No internet service'],[0,1,0],inplace=True)\n",
        "data['OnlineBackup'].replace(['No','Yes','No internet service'],[0,1,0],inplace=True)\n",
        "data['DeviceProtection'].replace(['No','Yes','No internet service'],[0,1,0],inplace=True)\n",
        "data['TechSupport'].replace(['No','Yes','No internet service'],[0,1,0],inplace=True)\n",
        "data['StreamingTV'].replace(['No','Yes','No internet service'],[0,1,0],inplace=True)\n",
        "data['StreamingMovies'].replace(['No','Yes','No internet service'],[0,1,0],inplace=True)\n",
        "data['Contract'].replace(['Month-to-month', 'One year', 'Two year'],[0,1,2],inplace=True)\n",
        "data['PaperlessBilling'].replace(['Yes','No'],[1,0],inplace=True)\n",
        "data['PaymentMethod'].replace(['Electronic check', 'Mailed check', 'Bank transfer (automatic)','Credit card (automatic)'],[0,1,2,3],inplace=True)\n",
        "data['Churn'].replace(['Yes','No'],[1,0],inplace=True)\n",
        "\n",
        "data.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LKxD7MsXnYjV"
      },
      "cell_type": "markdown",
      "source": [
        "Let's look at relationships between customer data and churn using correlation."
      ]
    },
    {
      "metadata": {
        "id": "uZh4954qnm0y"
      },
      "cell_type": "code",
      "source": [
        "corr = data.corr()\n",
        "sns.heatmap(corr, xticklabels=corr.columns.values, yticklabels=corr.columns.values, annot = True, annot_kws={'size':12})\n",
        "heat_map=plt.gcf()\n",
        "heat_map.set_size_inches(20,15)\n",
        "plt.xticks(fontsize=10)\n",
        "plt.yticks(fontsize=10)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Xu1DiQrfocI2"
      },
      "cell_type": "markdown",
      "source": [
        "Our goal is to avoid multicollinearity by dropping features that are closely correlated with each other. For example here it is TotalCharges and MonthlyCharges. So we will drop TotalCharges."
      ]
    },
    {
      "metadata": {
        "id": "vRIcVa-8ouOZ"
      },
      "cell_type": "code",
      "source": [
        "\tdata.pop('TotalCharges')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PV1tSYT6o7DS"
      },
      "cell_type": "code",
      "source": [
        "# Run info again to make sure TotalCharges has been dropped (popped off).\n",
        "data.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YiHnJp34Cvza"
      },
      "cell_type": "markdown",
      "source": [
        "Rerun corr chart after cleanup. TotalCharges should not appear in the corr chart."
      ]
    },
    {
      "metadata": {
        "id": "Ox5889JMCuiD"
      },
      "cell_type": "code",
      "source": [
        "corr = data.corr()\n",
        "sns.heatmap(corr, xticklabels=corr.columns.values, yticklabels=corr.columns.values, annot = True, annot_kws={'size':12})\n",
        "heat_map=plt.gcf()\n",
        "heat_map.set_size_inches(20,15)\n",
        "plt.xticks(fontsize=10)\n",
        "plt.yticks(fontsize=10)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "x2_BIVBZAOMs"
      },
      "cell_type": "markdown",
      "source": [
        "## 5: Explore The Data"
      ]
    },
    {
      "metadata": {
        "id": "JAyZt0XcANqa"
      },
      "cell_type": "code",
      "source": [
        "# Explore how many churn data points we have.\n",
        "print(len(data['Churn']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "S0WP1moGfjhg"
      },
      "cell_type": "code",
      "source": [
        "# Explore how many customers in this dataset have churned. Is this dataset 50% as the team suggests is the overall customer churn rate?\n",
        "data['Churn'].value_counts()\n",
        "# We see this dataset actually has less than the overall 50% churn rate of the entire company reported data (it's actually 26.54% that have churned."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_X08xUR8FRGH"
      },
      "cell_type": "code",
      "source": [
        "# This creates a bar graph of churn (Yes vs. No) so we can check how the data is balanced.\n",
        "data['Churn'].value_counts().plot(kind = 'bar', title = 'Bar Graph of Non-Churners vs Churners by Count (Churn is a 1)', color = 'blue', align = 'center')\n",
        "plt.show()\n",
        "# The dataset does not have a huge imbalance which is good news! But also we clearly see it does not have the 50% as we would have thought."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_KTJ2EeUHVGM"
      },
      "cell_type": "markdown",
      "source": [
        "Explore some contingencies on how some features relate to churn."
      ]
    },
    {
      "metadata": {
        "id": "QO8tgNqvHUve"
      },
      "cell_type": "code",
      "source": [
        "# Creates initial contingency table between Churn and gender. Male is 0, Female is 1.\n",
        "gender_churn_contingency = pd.crosstab(data[\"gender\"], data[\"Churn\"])\n",
        "display(gender_churn_contingency)\n",
        "# Male and females churn at about the same rate, so not much to see here. Let's keep moving."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TeatExD7H7fP"
      },
      "cell_type": "code",
      "source": [
        "# Check the data health. The sections should all be completely black indicating the data is complete.\n",
        "ms.matrix(data)\n",
        "# It looks good."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3_B7gx95IQKf"
      },
      "cell_type": "code",
      "source": [
        "# Explore the relationship between instances of Tech Support and Churn.\n",
        "# Stacked Bar of Tech Support and Churn.\n",
        "tech_support_churn = pd.crosstab(data['TechSupport'], data['Churn'])\n",
        "tech_support_churn.plot(kind = 'bar', stacked = True)\n",
        "plt.ylabel('Count')\n",
        "plt.xlabel('Tech Support Count')\n",
        "plt.title('Churn Rate Relative to Uses of Tech Support (Churned is a 1)')\n",
        "plt.show()\n",
        "# We can see that non-churners use tech support more often than customers that end up churning.\n",
        "# So let's explore some ways to get people to use Tech Support more often so they cancel (churn) less. You can see notes for this at the bottom.\n",
        "# Also, tech support in this data is just a Y/N. It would be useful in future to include how many tech support calls by customer so we could analyze how the number of tech support calls relates to churn."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "A8xHAhIJecrR"
      },
      "cell_type": "code",
      "source": [
        "# Churn rate relative to tenure.\n",
        "# Stacked bar of tenure and churn.\n",
        "tenure_churn = pd.crosstab(data['tenure'], data['Churn'])\n",
        "tenure_churn.plot(kind = 'bar', stacked = True)\n",
        "plt.ylabel('Count')\n",
        "plt.xlabel('Tenure of Subscription')\n",
        "plt.title('Churn Rate Relative to Tenure of Subscription (Churned is a 1)')\n",
        "plt.show()\n",
        "# We can clearly see the longer a customer stays as a subscriber, the less they are likely to churn!"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "onWnkXWVbFdA"
      },
      "cell_type": "code",
      "source": [
        "# Distribution of features.\n",
        "features = ['gender', 'SeniorCitizen','Partner', 'Dependents', 'tenure', 'PhoneService', 'MultipleLines', 'InternetService', 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies', 'Contract', 'PaperlessBilling', 'PaymentMethod', 'MonthlyCharges']\n",
        "data[features].describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IJ2CT6K0dNJq"
      },
      "cell_type": "code",
      "source": [
        "# Plot the distribution of observations for tenure.\n",
        "sns.distplot(data['tenure']);\n",
        "# It shows the max tenure is 70. This must be when the data history ends. We'll account for this in our analysis."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_UTsEzPeiQUt"
      },
      "cell_type": "code",
      "source": [
        "# Does how a customer pays have to do with their churn?\n",
        "_, axes = plt.subplots(1, 2, sharey=True, figsize=(10, 4))\n",
        "sns.countplot(x='PaperlessBilling', hue='Churn',\n",
        "              data=data, ax=axes[0]);\n",
        "sns.countplot(x='PaymentMethod', hue='Churn',\n",
        "              data=data, ax=axes[1]);\n",
        "# We can see that customers that use paperless billing are much more likely to churn (0 = don't have paperless billing). That seems backwards I would go check that data with the team.\n",
        "# We can see that customers that have the 0 payment method (electronic check) are much more likely to churn. Let's discourage that option."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Jd4Fpqwzlebv"
      },
      "cell_type": "code",
      "source": [
        "# See if the other products they have from this company has to do with their churn.\n",
        "_, axes = plt.subplots(1, 2, sharey=True, figsize=(10, 4))\n",
        "sns.countplot(x='PhoneService', hue='Churn',\n",
        "              data=data, ax=axes[0]);\n",
        "sns.countplot(x='InternetService', hue='Churn',\n",
        "              data=data, ax=axes[1]);\n",
        "# If they don't have Phone Service, they are more likely to churn.\n",
        "# If they don't have Internet Service, they are more likely to churn. Those customers with the highest Internet Service are least likely to churn.\n",
        "# Conclusion: This makes sense. Customers with other products from the company, and premium products, churn less.\n",
        "# Offer customers these additional products, perhaps even at a deep discount, so they take them and are less likely to churn."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Za0jp-Npy0ps"
      },
      "cell_type": "markdown",
      "source": [
        "## 6:  Prepare the Data"
      ]
    },
    {
      "metadata": {
        "id": "1INgdQZMy0pt"
      },
      "cell_type": "code",
      "source": [
        "# Splitting the data for testing and training."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OduTMX_ay0py"
      },
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(data.drop('Churn',axis=1),\n",
        "                                                    data['Churn'], test_size=0.30,\n",
        "                                                    random_state=101)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MhI4K6nby0p1"
      },
      "cell_type": "code",
      "source": [
        "train=pd.concat([X_train,y_train],axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ejwbdo3ny0p8"
      },
      "cell_type": "code",
      "source": [
        "# Function to estimate the best value of n_estimators and fit the model with the given data."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XDrWVKRSy0p_"
      },
      "cell_type": "code",
      "source": [
        "def modelfit(alg, dtrain, predictors,useTrainCV=True, cv_folds=5, early_stopping_rounds=50):\n",
        "\n",
        "    if useTrainCV:\n",
        "        #to get the parameters of xgboost\n",
        "        xgb_param = alg.get_xgb_params()\n",
        "\n",
        "        #to convert into a datastructure internally used by xgboost for training efficiency\n",
        "        # and speed\n",
        "        xgtrain = xgb.DMatrix(dtrain[predictors].values, label=dtrain[target].values)\n",
        "\n",
        "        #xgb.cv is used to find the number of estimators required for the parameters\n",
        "        # which are set\n",
        "        cvresult = xgb.cv(xgb_param, xgtrain,\n",
        "                          num_boost_round=alg.get_params()['n_estimators'], nfold=cv_folds,\n",
        "                        metrics='auc', early_stopping_rounds=early_stopping_rounds)\n",
        "\n",
        "        #setting the n_estimators parameter using set_params\n",
        "        alg.set_params(n_estimators=cvresult.shape[0])\n",
        "\n",
        "        print(alg.get_xgb_params())\n",
        "\n",
        "    #Fit the algorithm on the data\n",
        "    alg.fit(dtrain[predictors], dtrain['Churn'],eval_metric='auc')\n",
        "\n",
        "    return alg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AsILrXIay0qC"
      },
      "cell_type": "code",
      "source": [
        "# Function to get the accuracy of the model on the test data given the features considered."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "z9WG0lNYy0qF"
      },
      "cell_type": "code",
      "source": [
        "def get_accuracy(alg,predictors):\n",
        "    dtrain_predictions = alg.predict(X_test[predictors])\n",
        "    dtrain_predprob = alg.predict_proba(X_test[predictors])[:,1]\n",
        "    print (\"\\nModel Report\")\n",
        "    print (\"Accuracy : %.4g\" % metrics.accuracy_score(y_test.values,\n",
        "                                                      dtrain_predictions))\n",
        "    print (\"AUC Score (Train): %f\" % metrics.roc_auc_score(y_test.values,\n",
        "                                                           dtrain_predprob))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EBevLNUWy0qH"
      },
      "cell_type": "code",
      "source": [
        "# Function to get the feature importances based on the model fit."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AzV_bz3Hy0qK"
      },
      "cell_type": "code",
      "source": [
        "def get_feature_importances(alg):\n",
        "    #to get the feature importances based on xgboost we use fscore\n",
        "    feat_imp = pd.Series(alg._Booster.get_fscore()).sort_values(ascending=False)\n",
        "    print(feat_imp)\n",
        "\n",
        "    #this shows the feature importances on a bar chart\n",
        "    feat_imp.plot(kind='bar', title='Feature Importances')\n",
        "    plt.ylabel('Feature Importance Score')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4-96Qa11y0qQ"
      },
      "cell_type": "code",
      "source": [
        "target = 'Churn'\n",
        "IDcol = 'customerID'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xn7ABOQky0qV"
      },
      "cell_type": "markdown",
      "source": [
        "## 7: Model Selection, Predictions, and Metrics"
      ]
    },
    {
      "metadata": {
        "id": "qtqV9eggy0qW"
      },
      "cell_type": "code",
      "source": [
        "# To return the XGBClassifier object based on the values of the features."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9kBqFmhe0kWQ"
      },
      "cell_type": "code",
      "source": [
        "!pip install xgboost\n",
        "# XGBoost converts weak learners to strong learners through an ensemble method.\n",
        "# Unlike bagging, in the classical boosting the subset creation is not random and depends upon the performance of the previous models."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "swdjCkipy0qa"
      },
      "cell_type": "code",
      "source": [
        "def XgbClass(learning_rate =0.1,n_estimators=1000,max_depth=5,min_child_weight=1,\n",
        "             gamma=0,subsample=0.8,colsample_bytree=0.8):\n",
        "    xgb1 = XGBClassifier(learning_rate=learning_rate,\n",
        "                         n_estimators=n_estimators,\n",
        "                         max_depth=max_depth,\n",
        "                         min_child_weight=min_child_weight,\n",
        "                         gamma=gamma,\n",
        "                         subsample=subsample,\n",
        "                         colsample_bytree=colsample_bytree)\n",
        "    return xgb1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "m837XZX5y0qd"
      },
      "cell_type": "code",
      "source": [
        "# Function to return the list of predictors."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9dQWyI57y0qf"
      },
      "cell_type": "code",
      "source": [
        "# These are the initial parameters before tuning.\n",
        "def drop_features(l):\n",
        "    return [x for x in train.columns if x not in l]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jgoA5DT7y0qj"
      },
      "cell_type": "markdown",
      "source": [
        "### First Prediction: Use of initial parameters and without feature engineering"
      ]
    },
    {
      "metadata": {
        "id": "Qjz-FBOZ00Hn"
      },
      "cell_type": "code",
      "source": [
        "from xgboost import XGBClassifier\n",
        "import xgboost as xgb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yTNEx7Jfy0qj"
      },
      "cell_type": "code",
      "source": [
        "predictors = drop_features([target, IDcol])\n",
        "xgb1=XgbClass()\n",
        "first_model=modelfit(xgb1, train, predictors)\n",
        "xgb1.fit(train[predictors],train['Churn'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GpuBOp0Yy0qm"
      },
      "cell_type": "code",
      "source": [
        "get_accuracy(first_model,predictors)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2jOlGPtm07C2"
      },
      "cell_type": "markdown",
      "source": [
        "**Accuracy** is the proportion of true positives and negatives in the whole data set. It determines if a value is accurate compare it to the accepted value; the nearness of a calculation to the true value.\n",
        "\n",
        "**AUC** (area under the ROC receiver operating characteristic curve) measures how true positive rate (recall) and false positive rate trade off, so in that sense it is already measuring something else. More importantly, AUC is not a function of threshold. It is an evaluation of the classifier as threshold varies over all possible values. It is in a sense a broader metric, testing the quality of the internal value that the classifier generates and then compares to a threshold.\n",
        "\n",
        "**Accuracy vs AUC**: The accuracy depends on the threshold chosen, whereas the AUC considers all possible thresholds. Because of this it is often preferred as it provides a “broader” view of the performance of the classifier, but they still measure different things and as such using one or the other is problem-dependent."
      ]
    },
    {
      "metadata": {
        "id": "LTOwEH9iy0qp"
      },
      "cell_type": "code",
      "source": [
        "get_feature_importances(first_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "k7tki9U_y0qt"
      },
      "cell_type": "markdown",
      "source": [
        "### Second Prediction: Using intial Parameters and removing features of least importances"
      ]
    },
    {
      "metadata": {
        "id": "e8ICSpTry0qt"
      },
      "cell_type": "code",
      "source": [
        "# Model after removing the features of least importance."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UwzYOgb7y0qx"
      },
      "cell_type": "code",
      "source": [
        "dropl=['DeviceProtection','Dependents','Dependents','gender','StreamingMovies','MultipleLines']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2-MObariy0q0"
      },
      "cell_type": "code",
      "source": [
        "dropl_first=dropl+[target,IDcol]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VKroiJ2hy0q2"
      },
      "cell_type": "code",
      "source": [
        "# These are the initial parameters before tuning.\n",
        "predictors = drop_features(dropl_first)\n",
        "xgb1 = XgbClass()\n",
        "second_model=modelfit(xgb1, train, predictors)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0ny8g14-y0q5"
      },
      "cell_type": "code",
      "source": [
        "get_accuracy(second_model,predictors)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-qIp7Hsey0q8"
      },
      "cell_type": "code",
      "source": [
        "get_feature_importances(second_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YXh08Urby0q-"
      },
      "cell_type": "markdown",
      "source": [
        "### Third Prediction: Again removing the features of least importance"
      ]
    },
    {
      "metadata": {
        "id": "luJXI3key0q-"
      },
      "cell_type": "code",
      "source": [
        "dropl1=dropl+['Partner','PhoneService','OnlineBackup','TechSupport','OnlineSecurity']\n",
        "dropl_second=dropl_first+['Partner','PhoneService','OnlineBackup','TechSupport','OnlineSecurity']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mOO6UZLsy0rB"
      },
      "cell_type": "code",
      "source": [
        "predictors=drop_features(dropl_second)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "U5g3UZTFy0rD"
      },
      "cell_type": "code",
      "source": [
        "xgb1=XgbClass()\n",
        "third_model=modelfit(xgb1,train,predictors)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HRB8Q3wWy0rG"
      },
      "cell_type": "code",
      "source": [
        "get_accuracy(third_model,predictors)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BBMTnnlPy0rO"
      },
      "cell_type": "markdown",
      "source": [
        "## 8: Predict on New Cases"
      ]
    },
    {
      "metadata": {
        "id": "t21bqh6_y0rQ"
      },
      "cell_type": "code",
      "source": [
        "# Function stores the result in required csv file and features. I removed this section for now as the csv files are not yet saving correctly. See prototype version of notebook for details."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Alep7Q3Ky0rr"
      },
      "cell_type": "markdown",
      "source": [
        "## 9: Tuning"
      ]
    },
    {
      "metadata": {
        "id": "jyqfjclly0ru"
      },
      "cell_type": "code",
      "source": [
        "# Tune max_depth and min_child_weight."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3j-9y92iy0ry"
      },
      "cell_type": "code",
      "source": [
        "predictors = drop_features(dropl_first)\n",
        "predictors"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6L3sYX43y0r2"
      },
      "cell_type": "code",
      "source": [
        "param_test1 = {\n",
        " 'max_depth':list(range(5,10,1)),\n",
        " 'min_child_weight':list(range(5,10,1))\n",
        "}\n",
        "gsearch1 = GridSearchCV(estimator=XgbClass(n_estimators=48),param_grid =param_test1,\n",
        "                        scoring='roc_auc',n_jobs=-1,iid=False, cv=5, verbose=3)\n",
        "gsearch1.fit(train[predictors],train[target])\n",
        "gsearch1.cv_results_, gsearch1.best_params_, gsearch1.best_score_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3vc7mUGry0r6"
      },
      "cell_type": "code",
      "source": [
        "# If the best parameters are edge values then we do\n",
        "# GridSearchCV by taking one less and one value more than the best parameters."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3ToEbGG2y0sF"
      },
      "cell_type": "code",
      "source": [
        "param_test2 = {\n",
        " 'max_depth':[6,7,8,9],\n",
        " 'min_child_weight':[2,3,4,5]\n",
        "}\n",
        "gsearch2 = GridSearchCV(estimator=XgbClass(n_estimators=48),param_grid =param_test2,scoring='roc_auc',n_jobs=-1,iid=False, cv=5)\n",
        "gsearch2.fit(train[predictors],train[target])\n",
        "gsearch2.cv_results_, gsearch2.best_params_, gsearch2.best_score_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eBhirbgNy0sK"
      },
      "cell_type": "code",
      "source": [
        "xgb1 = XgbClass(max_depth=8,min_child_weight=4)\n",
        "model=modelfit(xgb1, train, predictors)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2AHAD8Lhy0sM"
      },
      "cell_type": "code",
      "source": [
        "get_accuracy(model,predictors)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "U6Yk0ZKWy0sO"
      },
      "cell_type": "code",
      "source": [
        "# To tune gamma."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "15q5_HFVy0sT"
      },
      "cell_type": "code",
      "source": [
        "param_test3 = {\n",
        " 'gamma':[i/10.0 for i in range(0,8)]\n",
        "}\n",
        "gsearch3=GridSearchCV(estimator=XgbClass(n_estimators=48,max_depth=7,min_child_weight=5),\n",
        "                      param_grid =param_test3,scoring='roc_auc',n_jobs=-1,iid=False, cv=5)\n",
        "gsearch3.fit(train[predictors],train[target])\n",
        "gsearch3.cv_results_, gsearch3.best_params_, gsearch3.best_score_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OU1BTcThy0sW"
      },
      "cell_type": "code",
      "source": [
        "xgb1 = XgbClass(max_depth=7,min_child_weight=5,gamma=0)\n",
        "model=modelfit(xgb1, train, predictors)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3gsJhBfAy0sb"
      },
      "cell_type": "code",
      "source": [
        "get_accuracy(model,predictors)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cyFX2Zovy0sb"
      },
      "cell_type": "code",
      "source": [
        "param_test4 = {\n",
        " 'subsample':[i/10.0 for i in range(6,10)],\n",
        " 'colsample_bytree':[i/10.0 for i in range(6,10)]\n",
        "}\n",
        "gsearch4=GridSearchCV(estimator=XgbClass(n_estimators=48,max_depth=7,\n",
        "                                         min_child_weight=5,gamma=0),\n",
        "                      param_grid =param_test4,scoring='roc_auc',n_jobs=-1,iid=False, cv=5)\n",
        "gsearch4.fit(train[predictors],train[target])\n",
        "gsearch4.cv_results_, gsearch4.best_params_, gsearch4.best_score_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WMPJK63Fy0sf"
      },
      "cell_type": "code",
      "source": [
        "xgb1 = XgbClass(max_depth=8,min_child_weight=4,gamma=0.4,subsample=0.8,colsample_bytree=0.6)\n",
        "model=modelfit(xgb1, train, predictors)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Vj-WHMJ9y0sg"
      },
      "cell_type": "code",
      "source": [
        "get_accuracy(model,predictors)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2hsrfoIXy0sj"
      },
      "cell_type": "code",
      "source": [
        "#dropl1=dropl+['Partner','PhoneService','OnlineBackup','TechSupport','OnlineSecurity']\n",
        "# RunTestAndSaveResults(dropl,'final_result.csv',model)\n",
        "#FixMeLater the final_result file is not printing to csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RDfFjAc2KCVs"
      },
      "cell_type": "code",
      "source": [
        "ls -l"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "15m86I98uHYC"
      },
      "cell_type": "markdown",
      "source": [
        "## 10: Next Steps\n",
        "* Run some actual data from your company. For case study purposes, let's keep going with dummy data ...\n",
        "* After some initial analysis, cleaning, and tuning we're consistently getting an accuracy of 80% and AUC ~84%. Feature engineering and tuning did not improve (in some cases made worse, seems to be overfitting).\n",
        "* This meets our prediction success threshold of 80%.\n",
        "* Share findings with team.\n",
        "* Discuss some possible next steps:\n",
        "    * Get more data. This is only ~7k records and much different than the stated overall 50%.\n",
        "    * Get more detailed data on features that are showing to be relatively MORE predictive:\n",
        "      * MonthlyCharges\n",
        "      * Tenure\n",
        "      * PaymentMethod\n",
        "      * Plan Types\n",
        "      \n",
        "  * **Get more detailed data on features that are showing to be relatively LESS predictive but our analysis show with more details they could be really useful**\n",
        "      * Tech support: how many calls to tech support lifetime and in past month since churning, instead of just a contact tech support: Y / N.\n",
        "  * **Get additional data on features we think may be very predictive but we don't have here yet**\n",
        "    * Time to first success point\n",
        "    * Other data specific to the business\n",
        "    * Etc.\n",
        "    \n",
        " ## Help Transform Customer Experience\n",
        "  * Iterate products to transform customer experiences:\n",
        "    * Personalized features (i.e if low NPS go to different flow)\n",
        "    * Data predictions (i.e. predict churn from TechSupport interaction and alert the customer support team to intervene, and automatically have the technology prompt the user for answering their questions),\n",
        "    * Propensity modeling (i.e set scorecards for customer predicted churn based on numerous factors. flag those users when they go 'red\" for action with marketing and customer case outreach).\n",
        "  \n",
        "  **Activity loop:**\n",
        "  * Iterate\n",
        "  * Communicate\n",
        "  * Put into production\n",
        "  * Scale\n",
        "  * Assess\n",
        "  * Repeat\n",
        "  \n",
        "  Questions?\n",
        "  Michelle Bonat\n",
        "  michelle.bonat@gmail.com\n",
        "  [michellebonat.com](http://michellebonat.com)"
      ]
    },
    {
      "metadata": {
        "id": "MDuedctmzhct"
      },
      "cell_type": "markdown",
      "source": [
        "## 11: Appendix"
      ]
    },
    {
      "metadata": {
        "id": "tmdd5wtazjuP"
      },
      "cell_type": "markdown",
      "source": [
        "## The initial data showed that more tech support interactions reduced customer churn. Here are some ways to put that information into action:\n",
        "To get customers interacting with tech support instead of cancelling:"
      ]
    },
    {
      "metadata": {
        "id": "pfwS5ZCV2lO7"
      },
      "cell_type": "markdown",
      "source": [
        "1) Include this NPS data in the dataset for analysis, it is from the questionnaire in QuickBooks."
      ]
    },
    {
      "metadata": {
        "id": "x060xZ642u5S"
      },
      "cell_type": "markdown",
      "source": [
        "2) Make sure screens/flow to include links to Support during the Cancel process."
      ]
    },
    {
      "metadata": {
        "id": "GhUFDTzp3H78"
      },
      "cell_type": "markdown",
      "source": [
        "3) Offer users a way to get help even live chat right there instead of just a cancel option."
      ]
    },
    {
      "metadata": {
        "id": "J4S8hGxn3mXw"
      },
      "cell_type": "markdown",
      "source": [
        "4) There could also be a 'Resolve Your Issue' link right next to the cancel option."
      ]
    }
  ]
}